{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Some Libraries\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadModel:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        layers = model_data['layers']\n",
    "        n_layers = len(layers)\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.activations = []\n",
    "        self.sigmoid_gradient = lambda x: x * (1 - x)\n",
    "        self.relu_gradient = lambda x: 1. * (x > 0)\n",
    "        self.tanh_gradient = lambda x: 1 - np.tanh(x)**2\n",
    "        self.softmax_gradient = lambda x: x * (1 - x)\n",
    "        \n",
    "        for i in range(1, n_layers):\n",
    "            layer = layers[i]\n",
    "            \n",
    "            if layer['tipe'] == 'hidden':\n",
    "                n_neurons = layer['n_neuron']\n",
    "                weight = np.array(layer['weight'])\n",
    "                bias = np.array(layer['bias'])\n",
    "                activation = layer['activation_function']\n",
    "                \n",
    "                self.weights.append(weight)\n",
    "                self.biases.append(bias)\n",
    "                self.activations.append(activation)\n",
    "                \n",
    "            elif layer['tipe'] == 'output':\n",
    "                n_neurons = layer['n_neuron']\n",
    "                weight = np.array(layer['weight'])\n",
    "                bias = np.array(layer['bias'])\n",
    "                activation = layer['activation']\n",
    "                \n",
    "                self.weights.append(weight)\n",
    "                self.biases.append(bias)\n",
    "                self.activations.append(activation)\n",
    "                \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def get_biases(self):\n",
    "        return self.biases\n",
    "    \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "    \n",
    "    def get_layer(self, layer):\n",
    "        return self.weights[layer], self.biases[layer], self.activations[layer]\n",
    "    \n",
    "    def get_layer_weights(self, layer):\n",
    "        return self.weights[layer]\n",
    "    \n",
    "    def get_layer_biases(self, layer):\n",
    "        return self.biases[layer]\n",
    "    \n",
    "    def get_layer_activations(self, layer):\n",
    "        return self.activations[layer]\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.weights, self.biases, self.activations\n",
    "    \n",
    "    def get_activation_gradient(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: 1. * (x > 0)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: x * (1. - x)\n",
    "        elif activation == 'linear':\n",
    "            return lambda x: np.ones_like(x)\n",
    "        elif activation == 'softmax':\n",
    "            return lambda x: x * (1. - x)\n",
    "\n",
    "    \n",
    "    # print model per layer\n",
    "    def print_model(self):\n",
    "        for i in range(len(self.weights)):\n",
    "            print('Layer', i+1)\n",
    "            print('Weights:', self.weights[i])\n",
    "            print('Biases:', self.biases[i])\n",
    "            print('Activations:', self.activations[i])\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed Forward\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,2) and (4,2) not aligned: 2 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19920/3083230123.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19920/3083230123.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Feed Forward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19920/3083230123.py\u001b[0m in \u001b[0;36mfeed_forward\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,2) and (4,2) not aligned: 2 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "class BackPropagation:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = LoadModel(model_path)\n",
    "        self.weights = self.model.get_weights()\n",
    "        self.biases = self.model.get_biases()\n",
    "        self.activations = self.model.get_activations()\n",
    "        self.sigmoid_gradient = lambda x: x * (1 - x)\n",
    "        self.relu_gradient = lambda x: 1. * (x > 0)\n",
    "        self.tanh_gradient = lambda x: 1 - np.tanh(x)**2\n",
    "        self.softmax_gradient = lambda x: x * (1 - x)\n",
    "\n",
    "    def get_activation_gradient(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return lambda x: 1. * (x > 0)\n",
    "        elif activation == 'sigmoid':\n",
    "            return lambda x: x * (1. - x)\n",
    "        elif activation == 'linear':\n",
    "            return lambda x: np.ones_like(x)\n",
    "        elif activation == 'softmax':\n",
    "            return lambda x: x * (1. - x)\n",
    "        \n",
    "    def feed_forward(self, input_data):\n",
    "        input_data = np.array(input_data)\n",
    "        n_layers = len(self.weights)\n",
    "        \n",
    "        self.z = []\n",
    "        self.a = []\n",
    "        \n",
    "        self.a.append(input_data)\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            z = np.dot(self.weights[i], self.a[i]) + self.biases[i]\n",
    "            self.z.append(z)\n",
    "            \n",
    "            if i == n_layers-1:\n",
    "                a = self.softmax(z)\n",
    "            else:\n",
    "                a = self.relu(z)\n",
    "                \n",
    "            self.a.append(a)\n",
    "            \n",
    "        return self.a[-1]\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def back_propagation(self, input_data, target):\n",
    "        input_data = np.array(input_data)\n",
    "        target = np.array(target)\n",
    "        \n",
    "        n_layers = len(self.weights)\n",
    "        \n",
    "        self.feed_forward(input_data)\n",
    "        \n",
    "        delta = []\n",
    "        nabla_w = []\n",
    "        nabla_b = []\n",
    "        \n",
    "        # output layer\n",
    "        delta.append(self.a[-1] - target)\n",
    "        nabla_w.append(np.dot(delta[-1], self.a[-2].T))\n",
    "        nabla_b.append(delta[-1])\n",
    "        \n",
    "        # hidden layer\n",
    "        for i in range(n_layers-2, -1, -1):\n",
    "            delta.append(np.dot(self.weights[i+1].T, delta[-1]) * self.relu_gradient(self.a[i+1]))\n",
    "            nabla_w.append(np.dot(delta[-1], self.a[i].T))\n",
    "            nabla_b.append(delta[-1])\n",
    "            \n",
    "        nabla_w.reverse()\n",
    "        nabla_b.reverse()\n",
    "        \n",
    "        return nabla_w, nabla_b\n",
    "    \n",
    "    def update_weights(self, nabla_w, nabla_b, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * nabla_w[i]\n",
    "            self.biases[i] -= learning_rate * nabla_b[i]\n",
    "\n",
    "def main():\n",
    "    model_path = 'relu.json'\n",
    "    backprop = BackPropagation(model_path)\n",
    "    \n",
    "    input_data = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "    target = [0, 1, 1, 0]\n",
    "    \n",
    "    print('Feed Forward')\n",
    "    print(backprop.feed_forward(input_data))\n",
    "    print('')\n",
    "    \n",
    "    print('Back Propagation')\n",
    "    nabla_w, nabla_b = backprop.back_propagation(input_data, target)\n",
    "    print('Nabla W:', nabla_w)\n",
    "    print('Nabla B:', nabla_b)\n",
    "    print('')\n",
    "    \n",
    "    print('Update Weights')\n",
    "    backprop.update_weights(nabla_w, nabla_b, 0.1)\n",
    "    print('Weights:', backprop.weights)\n",
    "    print('Biases:', backprop.biases)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
